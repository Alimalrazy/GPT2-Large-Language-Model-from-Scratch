{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2Acxfn0Pm8Z"
      },
      "source": [
        "#Build LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g4zN1fa9Pm8b"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    'vocab_size': 50257,   # Vucabulary Size\n",
        "    'ctx_len': 265,       # Context Lenghts\n",
        "    'emb_dim': 768,        # Embedding dimension\n",
        "    'n_heads': 12,         # Number of attention head\n",
        "    'n_layers': 12,        # number of Layer\n",
        "    'drop_rate': 0.1,      # Dropout rate\n",
        "    'qkv_bias': False      # Query_Key_value Bias\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aOGYmnNPm8e"
      },
      "source": [
        "## Tokenizing text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhNn3NNWPm8e",
        "outputId": "8cca9751-43ff-4f4f-a639-7160a4d096f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VfBk230XPm8g"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KOkzs7WwPm8g"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True):\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c0dQQYQPm8h"
      },
      "source": [
        "we first define a train_ratio to use 90% of the data for training and the remaining 10% as validation data for model evaluation during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IRm8J2mNPm8h"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text_data = f.read()\n",
        "\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"ctx_len\"],\n",
        "    stride=GPT_CONFIG_124M[\"ctx_len\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True\n",
        ")\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"ctx_len\"],\n",
        "    stride=GPT_CONFIG_124M[\"ctx_len\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcgj3j4RPm8i",
        "outputId": "0fd104c2-50ee-4a6e-f1c7-6979796c883b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader:\n",
            "torch.Size([2, 265]) torch.Size([2, 265])\n",
            "torch.Size([2, 265]) torch.Size([2, 265])\n",
            "torch.Size([2, 265]) torch.Size([2, 265])\n",
            "torch.Size([2, 265]) torch.Size([2, 265])\n",
            "torch.Size([2, 265]) torch.Size([2, 265])\n",
            "torch.Size([2, 265]) torch.Size([2, 265])\n",
            "torch.Size([2, 265]) torch.Size([2, 265])\n",
            "torch.Size([2, 265]) torch.Size([2, 265])\n",
            "\n",
            "Validation loader:\n",
            "torch.Size([2, 265]) torch.Size([2, 265])\n"
          ]
        }
      ],
      "source": [
        "print(\"Train loader:\")\n",
        "for x, y in train_loader:\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "print(\"\\nValidation loader:\")\n",
        "for x, y in val_loader:\n",
        "    print(x.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvEMiHF3Pm8i"
      },
      "source": [
        "#Coding Attention Mechanisms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtinGpdVPm8i"
      },
      "source": [
        "## A layer Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QXHAM2HFPm8j"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "w6qMhzGDPm8k"
      },
      "outputs": [],
      "source": [
        "class GELU(nn.Module):\n",
        "    def __init__(self,):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XaAr_IqiPm8l"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg['emb_dim'], 4 * cfg['emb_dim']),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg['emb_dim'], cfg['emb_dim']),\n",
        "            nn.Dropout(cfg['drop_rate'])\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF6eDvLKPm8m"
      },
      "source": [
        "## Multi-head attention layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rA4QTNdlPm8m"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, block_size, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0 # d_out must be divisible by n_heads\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            'mask',\n",
        "            torch.triu(torch.ones(block_size, block_size), diagonal=1)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        queries = self.W_query(x)\n",
        "        keys = self.W_key(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(2, 3)\n",
        "\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "        mask_unsqueezed = mask_bool.unsqueeze(0).unsqueeze(0)\n",
        "        attn_scores.masked_fill_(mask_unsqueezed, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
        "\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)\n",
        "\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPC9VF4lPm8o"
      },
      "source": [
        "## The transformer block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "R1JWdzjaPm8o"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg['emb_dim'],\n",
        "            d_out=cfg['emb_dim'],\n",
        "            block_size=cfg['ctx_len'],\n",
        "            num_heads=cfg['n_heads'],\n",
        "            dropout=cfg['drop_rate'],\n",
        "            qkv_bias=cfg['qkv_bias'],\n",
        "        )\n",
        "\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg['emb_dim'])\n",
        "        self.norm2 = LayerNorm(cfg['emb_dim'])\n",
        "        self.drop_resid = nn.Dropout(cfg['drop_rate'])\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)\n",
        "        x = self.drop_resid(x)\n",
        "        x = x + shortcut # Add the original input back\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_resid(x)\n",
        "        x = x + shortcut\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aq3GrZTPm8p"
      },
      "source": [
        "#Build GPT-2 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KL1Nbo6NPm8p"
      },
      "outputs": [],
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tok_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
        "        self.pos_emb = nn.Embedding(cfg['ctx_len'], cfg['emb_dim'])\n",
        "        self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg['n_layers'])]\n",
        "        )                                                                                   #A\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg['emb_dim'])                                        #B\n",
        "        self.out_head = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds\n",
        "\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTNfL5LwPm8q"
      },
      "source": [
        "## Analysis Model Architecture and Size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TchpUqW5Pm8q"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icWco9Q9Pm8r",
        "outputId": "6702b93a-73b7-495b-ed71-ec25545b10f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 162,426,624\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5l5y8aaPm8t",
        "outputId": "10b8ee86-7c15-4cc0-df5b-7ce3afc60543"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token embedding layer shape: torch.Size([50257, 768])\n",
            "Output layer shape: torch.Size([50257, 768])\n"
          ]
        }
      ],
      "source": [
        "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
        "print(\"Output layer shape:\", model.out_head.weight.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCSQHzarPm8t"
      },
      "source": [
        "The token embedding and output layers are very large due to the number of rows for the 50,257 in the tokenizer's vocabulary. Let's remove the output layer parameter count from the total GPT-2 model count according to the weight tying:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUG0qFYXPm8u",
        "outputId": "7a18dc04-ea1b-4e93-8e6b-6e2d8e33098f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters considering weight tying: 123,829,248\n"
          ]
        }
      ],
      "source": [
        "total_params_gpt2 =  total_params - sum(p.numel() for p in model.out_head.parameters())\n",
        "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttf25_QHPm8u",
        "outputId": "233eb6e4-a892-41fe-ca51-e46f10f22715"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total size of the model: 619.61 MB\n"
          ]
        }
      ],
      "source": [
        "total_size_bytes = total_params * 4\n",
        "total_size_mb = total_size_bytes / (1024 * 1024)\n",
        "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hDkkiTXyPm8v"
      },
      "outputs": [],
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "        if top_k is not None:\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(\n",
        "                logits < min_val,\n",
        "                torch.tensor(float('-inf')).to(logits.device),\n",
        "                logits\n",
        "            )\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0ECHdP_dPm8w"
      },
      "outputs": [],
      "source": [
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZEv03dLPm8w",
        "outputId": "25526934-ef64-4239-fac5-ef4bc2599fc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves youilion diversebull incurabies relativity ° overtly Elaine legalization Pirates voyage Prior banning bur\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
        "    max_new_tokens=15,\n",
        "    context_size=GPT_CONFIG_124M[\"ctx_len\"],\n",
        "    top_k=25,\n",
        "    temperature=1.4\n",
        ")\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axVFHxggPm8x"
      },
      "source": [
        "#Pretraining on Unlabeled Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9VGmV39vPm8y"
      },
      "outputs": [],
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(\n",
        "        logits.flatten(0, 1), target_batch.flatten()\n",
        "    )\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "SucF2epPPm8y"
      },
      "outputs": [],
      "source": [
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ZmC4NBmrPm8z"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4oStFq0GPm8z"
      },
      "outputs": [],
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size,\n",
        "            top_k=25,\n",
        "            temperature=1.4\n",
        "        )\n",
        "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "PVLAcFBMPm80"
      },
      "outputs": [],
      "source": [
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context):\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        generate_and_print_sample(\n",
        "            model, train_loader.dataset.tokenizer, device, start_context\n",
        "        )\n",
        "    return train_losses, val_losses, track_tokens_seen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdZNODJoPm81",
        "outputId": "d0d1d457-51b4-4ee7-969f-263e2cce9b74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 9.843, Val loss 10.018\n",
            "Ep 1 (Step 000005): Train loss 7.869, Val loss 8.365\n",
            "Every effort moves you to the \" it.      he it had--\"   his had it    and     and I, that, that--.   \" the had I the, the I  \n",
            "Ep 2 (Step 000010): Train loss 6.850, Val loss 7.171\n",
            "Ep 2 (Step 000015): Train loss 5.908, Val loss 6.724\n",
            "Every effort moves you a was had he of-- was. that the that and him was the\" was my\" had the. , his.\"'t him in the of a with in that the it and the I in with. \" was,,,.\n",
            "Ep 3 (Step 000020): Train loss 5.894, Val loss 6.589\n",
            "Every effort moves you in the in it him to I\"I of a the Iis in of. G, and theI thatburn was. The a I of-- G. and I that, he it \". had G him to with of-- was\n",
            "Ep 4 (Step 000025): Train loss 5.524, Val loss 6.548\n",
            "Ep 4 (Step 000030): Train loss 5.653, Val loss 6.708\n",
            "Every effort moves you his a he he.'s my me-- he of of a me., in he was that and-- not-- with the had it the my you it at in he had his not you a to him he was was on the in him.\n",
            "Ep 5 (Step 000035): Train loss 5.111, Val loss 6.556\n",
            "Every effort moves you\" a, and in the a to--it--I was I had been through theisburn was to now I to always was he had to Mrs. I had in the by, with Gis, in the house a little to the Mrs\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "num_epochs = 5\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=1,\n",
        "    start_context=\"Every effort moves you\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "3-Qi0eCaPm82",
        "outputId": "487b3858-e2e4-4aa6-fce9-9a44506a6169"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax2 = ax1.twiny()\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qk2C2YeOPm84",
        "outputId": "bd18ae53-0a8b-45c7-9ab3-f4eb108b6d8a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(265, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (3): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (3): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (3): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (3): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (3): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (3): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (3): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (3): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (3): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (3): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (3): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (3): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "model.to(\"cpu\")\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQS37qTXWkNS",
        "outputId": "9361a2a2-8450-490b-f9aa-564b59b94d4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your unfinished text: Who are\n",
            "Output text:\n",
            " Who are.\n",
            "\" a was his, I was was, and I had a in the--and I had to have a, I had been the a he was his pictures--and the house--and the\n",
            "\"I, I had to have I\n"
          ]
        }
      ],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# Use input() function to get the text from the user\n",
        "input_text = input(\"Enter your unfinished text: \")\n",
        "\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(input_text, tokenizer),\n",
        "    max_new_tokens=50,\n",
        "    context_size=GPT_CONFIG_124M[\"ctx_len\"],\n",
        "    top_k=50,\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 5045113,
          "sourceId": 8463029,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30699,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}